{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predicting flight delays by weather data__\n",
    "\n",
    "The goal of this notebook is to present a comprehensive analysis conducted as part of the \"Python for Data Scientists\" course at ENSAE. This project  includes data collection and preprocessing, visualization, scraping and modeling. By combining exploratory analysis and machine learning, the notebook aims to provide actionable insights into how weather affects flight schedules, focusing specifically on data from JFK Airport in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation and Problematic**\n",
    "\n",
    "Flight delays are a common inconvenience for both passengers and airlines, and while many are attributed to operational or logistical issues, weather often plays a crucial role. However, understanding and predicting these delays using weather data alone poses significant challenges.\n",
    "\n",
    "This project seeks to address the following key questions:\n",
    "- How do weather conditions correlate with flight delays?\n",
    "- Can weather data reliably predict delays?\n",
    "- What are the limitations of weather-based predictive models in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Executive summary**\n",
    "TO BE COMPLETED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data collection:**\n",
    "\n",
    "On the web, we found two useful datasets to build our algorithm.\n",
    "1. *Plane data*:\n",
    "   - **[Bureau of Transportation Statistics](https://www.transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGJ&QO_fu146_anzr=b0-gvzr)**: Dataset containing detailed information about flights, like delays and destinations. The website interface enables to download precisely the data we want, from the variables to the period of time considered (year and month). We decided to restrict ourselves to download the data from the year 2017 only, as it takes a lot of time to download a dataset for a single month. \n",
    "\n",
    "2. *Weather data*:\n",
    "   - **[IBM Weather Data](https://developer.ibm.com/exchanges/data/all/jfk-weather-data/)**: Weather data from JFK airport during the year 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of hypotheses**\n",
    "1. A plane's delays are similarly sensitive to weather variations. We can generalize the situation for an airport to others\n",
    "2. We can replaces the missing values for delays by 0 because we suppose that a delay is more likely to be registered than no delay as it creates more frustration\n",
    "3. 'T' is used to indicate a quantity observed was too low to be measured, we assume it is equal to zero\n",
    "4. The weather is the same for each 30-min interval of time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the project**\n",
    "\n",
    "1. Data Cleaning\n",
    "2. Exploratory Data Analysis\n",
    "3. Modeling (ML)\n",
    "4. Interpretation and Insights\n",
    "5. Application that predicts flight delays with the previous models\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installations and Prerequisites**\n",
    "\n",
    "Before running the notebook, ensure that the required Python modules are installed to guarantee smooth execution of the code. Execute the following command in your terminal or notebook cell to install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Useful Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Data cleaning**\n",
    "\n",
    "First, we download the files from leoacpr by typing the SSP Cloud username. If you do this step, please note that this can take a while because the files are heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_april.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_august.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_december.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_february.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_january.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_july.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_june.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_march.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_may.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_november.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_october.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/T_ONTIME_REPORTING_september.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/df.csv\n",
      "Downloading leoacpr/diffusion/Pre-processing/jfk_weather.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22411/102535606.py:36: DtypeWarning: Columns (10,12,13,14,15,16,17,18,19,22,24,25,27,42,65,66,69,70,71,86,87,88,89) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_imported = pd.read_csv(file_in)\n"
     ]
    }
   ],
   "source": [
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "MY_BUCKET = \"leoacpr\"\n",
    "source_folder = f\"{MY_BUCKET}/diffusion/Pre-processing\"\n",
    "\n",
    "YOUR_BUCKET = str(input(\"Type your bucket: \\n\"))\n",
    "\n",
    "source_folder = f\"{MY_BUCKET}/diffusion/Pre-processing\"\n",
    "files_in_source = fs.ls(source_folder)\n",
    "\n",
    "# Copying the dataframe from leoacpr to your s3 database\n",
    "for file_path_in_s3 in files_in_source:\n",
    "    file_name = file_path_in_s3.split('/')[-1]  # Name of the file without the path\n",
    "\n",
    "    # If the file already exists in your database, then it won't download it\n",
    "    if fs.ls(f\"{MY_BUCKET}/diffusion/Pre-processing\") != fs.ls(f\"{YOUR_BUCKET}/diffusion/Pre-processing\"):\n",
    "        file_path_for_you = f\"{YOUR_BUCKET}/diffusion/Pre-processing/{file_name}\"\n",
    "        #import\n",
    "        with fs.open(file_path_in_s3, \"r\") as file_in:\n",
    "            df_imported = pd.read_csv(file_in)\n",
    "        #export\n",
    "        with fs.open(file_path_for_you, \"w\") as file_out:\n",
    "            df_imported.to_csv(file_out)\n",
    "        \n",
    "        print(f\"File {file_name} has been successfully copied to {file_path_for_you}\")\n",
    "\n",
    "#Create folders inside S3\n",
    "if not fs.exists(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data\"):\n",
    "    fs.touch(f\"{YOUR_BUCKET}/diffusion/.{Pre-processed_data}]\")\n",
    "\n",
    "#Downloading the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "for files in fs.ls(f\"{YOUR_BUCKET}/diffusion/Pre-processing\"):\n",
    "    with fs.open(files, \"r\") as file_in:\n",
    "            df_imported = pd.read_csv(file_in)\n",
    "            print(f\"Downloading {files}\")\n",
    "    # Dictionnary of dataframes with the name of the file as a key\n",
    "    dataframes[f\"{files.split('/')[-1]}\"] = df_imported\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will be useful after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_columns(df):\n",
    "    \"\"\"Checks and prints the columns containing NaN values\n",
    "\n",
    "    Args:\n",
    "        df (pd.dataframe): a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    nan_columns = df.columns[df.isna().any()].tolist()\n",
    "    for col in nan_columns:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        print(f\"Colonne '{col}' contient {nan_count} valeurs NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing the planes data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22411/1635564088.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  JFK_2017['FL_DATE'] = pd.to_datetime(JFK_2017['FL_DATE'])\n",
      "/tmp/ipykernel_22411/1635564088.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  JFK_2017_number['CANCELLED'] = JFK_2017_number['CANCELLED'].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne 'DEP_TIME' contient 38 valeurs NaN.\n",
      "Colonne 'ARR_TIME' contient 41 valeurs NaN.\n"
     ]
    }
   ],
   "source": [
    "#Merging the monthly datasets to obtain a dataset for 2017\n",
    "january_JFK = dataframes['T_ONTIME_REPORTING_january.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "february_JFK = dataframes['T_ONTIME_REPORTING_february.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "march_JFK = dataframes['T_ONTIME_REPORTING_march.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "april_JFK = dataframes['T_ONTIME_REPORTING_april.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "may_JFK = dataframes['T_ONTIME_REPORTING_may.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "june_JFK = dataframes['T_ONTIME_REPORTING_june.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "july_JFK = dataframes['T_ONTIME_REPORTING_july.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "august_JFK = dataframes['T_ONTIME_REPORTING_august.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "september_JFK = dataframes['T_ONTIME_REPORTING_september.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "october_JFK = dataframes['T_ONTIME_REPORTING_october.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "november_JFK = dataframes['T_ONTIME_REPORTING_november.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "december_JFK = dataframes['T_ONTIME_REPORTING_december.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "#Total size = 2138\n",
    "year = [january_JFK, february_JFK, march_JFK, april_JFK, may_JFK, june_JFK, july_JFK, august_JFK, september_JFK, october_JFK, november_JFK, december_JFK]\n",
    "JFK_2017 = pd.concat(year, ignore_index=True)\n",
    "JFK_2017.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "#print(JFK_2017)\n",
    "\n",
    "#Setting the rights data types\n",
    "#print(JFK_2017.info())\n",
    "JFK_2017['FL_DATE'] = pd.to_datetime(JFK_2017['FL_DATE'])\n",
    "\n",
    "#Hypothesis : we can replaces the missing values for delays by 0 because we suppose that a delay is more likely to be registered than no delay because it creates more frustration\n",
    "JFK_2017['WEATHER_DELAY'] = JFK_2017['WEATHER_DELAY'].fillna(0)\n",
    "JFK_2017['DEP_DELAY'] = JFK_2017['DEP_DELAY'].fillna(0)\n",
    "JFK_2017['CARRIER_DELAY'] = JFK_2017['CARRIER_DELAY'].fillna(0)\n",
    "JFK_2017['WEATHER_DELAY'] = JFK_2017['WEATHER_DELAY'].fillna(0)\n",
    "JFK_2017['ARR_DELAY'] = JFK_2017['ARR_DELAY'].fillna(0)\n",
    "\n",
    "#Removing NaN per rows\n",
    "check_nan_columns(JFK_2017)\n",
    "#'DEP_TIME' has 36  NaN.\n",
    "#'ARR_TIME' has 39 valeurs NaN.\n",
    "JFK_2017 = JFK_2017.dropna(axis=0)\n",
    "check_nan_columns(JFK_2017) #nothing --> no more NaN\n",
    "#print(len(JFK_2017)) \n",
    "#1919\n",
    "\n",
    "#Combining departure time information to obtain a column with year, month, day, hour, minute for departure\n",
    "#'DEP_TIME' contains str with numbers indicating the hours and minutes through the format \"hhmm\"\n",
    "JFK_2017['DEP_TIME'] = JFK_2017['DEP_TIME'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "JFK_2017['DEP_TIME'] = JFK_2017['DEP_TIME'].astype(str).str.zfill(4)  # Ensure it's 4 digits\n",
    "JFK_2017['Hours'] = JFK_2017['DEP_TIME'].str[:2].astype(int)  # Get hours as integer\n",
    "JFK_2017['Minutes'] = JFK_2017['DEP_TIME'].str[2:].astype(int)  # Get minutes as integer\n",
    "JFK_2017['departure_time'] = pd.to_timedelta(JFK_2017['Hours'], unit='h') + pd.to_timedelta(JFK_2017['Minutes'], unit='m')\n",
    "JFK_2017['Full_Departure_Datetime'] = JFK_2017['FL_DATE'] + JFK_2017['departure_time']\n",
    "JFK_2017.drop(['Hours', 'Minutes', 'departure_time'], axis=1, inplace=True)\n",
    "#print(JFK_2017[['FL_DATE', 'DEP_TIME', 'Full_Departure_Datetime']].head())\n",
    "\n",
    "#Isolating the data for machine learning \n",
    "JFK_2017_no_number = JFK_2017[['Full_Departure_Datetime', 'FL_DATE','OP_UNIQUE_CARRIER','OP_CARRIER_AIRLINE_ID','OP_CARRIER','TAIL_NUM','OP_CARRIER_FL_NUM','ORIGIN_AIRPORT_ID','ORIGIN_AIRPORT_SEQ_ID','ORIGIN_CITY_MARKET_ID','DEST_AIRPORT_ID','DEST_CITY_MARKET_ID','DEST', 'DEP_TIME','ARR_TIME']]\n",
    "JFK_2017_number = JFK_2017[['DEP_DELAY','ARR_DELAY','CANCELLED','CARRIER_DELAY','WEATHER_DELAY','Full_Departure_Datetime']]\n",
    "JFK_2017_number['CANCELLED'] = JFK_2017_number['CANCELLED'].astype(int)\n",
    "#print(JFK_2017_number.info())\n",
    "\n",
    "#Exporting the dataset for JFK planes\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/JFK_2017.csv\", \"w\") as path:\n",
    "    JFK_2017.to_csv(path)\n",
    "\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/JFK_2017_no_number.csv\", \"w\") as path:\n",
    "    JFK_2017_no_number.to_csv(path)\n",
    "\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/JFK_2017_number.csv\", \"w\") as path:\n",
    "    JFK_2017_number.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing the weather data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22411/3998382021.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_2017['YearMonth'] = weather_2017['DATE'].dt.to_period('M')  # Extraire l'année et le mois\n",
      "/tmp/ipykernel_22411/3998382021.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  weather_2017['YearDayHour'] = weather_2017['DATE'].dt.to_period('H')  # Extraire l'année, jour et heure\n",
      "/tmp/ipykernel_22411/3998382021.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_2017['YearDayHour'] = weather_2017['DATE'].dt.to_period('H')  # Extraire l'année, jour et heure\n",
      "/tmp/ipykernel_22411/3998382021.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_2017['YearDay'] = weather_2017['DATE'].dt.to_period('D')  # Extraire l'année et jour\n",
      "/tmp/ipykernel_22411/3998382021.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df.groupby(period_key)[col].transform(lambda group: group.ffill().bfill())\n",
      "/tmp/ipykernel_22411/3998382021.py:35: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df.groupby(period_key)[col].transform(lambda group: group.ffill().bfill())\n",
      "/tmp/ipykernel_22411/3998382021.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_2017.drop(columns=['YearMonth', 'YearDayHour', 'YearDay'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne 'HOURLYSKYCONDITIONS' contient 5 valeurs NaN.\n",
      "Colonne 'HOURLYVISIBILITY' contient 5 valeurs NaN.\n",
      "Colonne 'HOURLYPRSENTWEATHERTYPE' contient 10553 valeurs NaN.\n",
      "Colonne 'HOURLYWETBULBTEMPF' contient 13 valeurs NaN.\n",
      "Colonne 'HOURLYWETBULBTEMPC' contient 13 valeurs NaN.\n",
      "Colonne 'HOURLYDewPointTempF' contient 4 valeurs NaN.\n",
      "Colonne 'HOURLYDewPointTempC' contient 4 valeurs NaN.\n",
      "Colonne 'HOURLYRelativeHumidity' contient 4 valeurs NaN.\n",
      "Colonne 'HOURLYWindGustSpeed' contient 11048 valeurs NaN.\n",
      "Colonne 'HOURLYStationPressure' contient 9 valeurs NaN.\n",
      "Colonne 'HOURLYPressureTendency' contient 4379 valeurs NaN.\n",
      "Colonne 'HOURLYPressureChange' contient 7324 valeurs NaN.\n",
      "Colonne 'HOURLYSeaLevelPressure' contient 2 valeurs NaN.\n",
      "Colonne 'HOURLYPrecip' contient 5 valeurs NaN.\n",
      "Colonne 'DAILYAverageRelativeHumidity' contient 1415 valeurs NaN.\n",
      "Colonne 'DAILYAverageDewPointTemp' contient 1016 valeurs NaN.\n",
      "Colonne 'DAILYAverageWetBulbTemp' contient 1016 valeurs NaN.\n",
      "Colonne 'DAILYWeather' contient 6145 valeurs NaN.\n",
      "Colonne 'DAILYAverageSeaLevelPressure' contient 1016 valeurs NaN.\n",
      "Colonne 'DAILYAverageWindSpeed' contient 42 valeurs NaN.\n",
      "Colonne 'DAILYPeakWindSpeed' contient 115 valeurs NaN.\n",
      "Colonne 'PeakWindDirection' contient 12839 valeurs NaN.\n",
      "Colonne 'MonthlyAverageRH' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyDewpointTemp' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyWetBulbTemp' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyAvgHeatingDegreeDays' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyAvgCoolingDegreeDays' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyAverageWindSpeed' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyTotalSnowfall' contient 7791 valeurs NaN.\n",
      "Colonne 'MonthlyGreatestPrecip' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyGreatestPrecipDate' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyGreatestSnowfallDate' contient 7791 valeurs NaN.\n",
      "Colonne 'MonthlyGreatestSnowDepthDate' contient 8776 valeurs NaN.\n",
      "Colonne 'MonthlyDaysWithGT001Precip' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyDaysWithGT010Precip' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyDaysWithGT1Snow' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyMaxSeaLevelPressureValue' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyMaxSeaLevelPressureDate' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyMaxSeaLevelPressureTime' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyMinSeaLevelPressureValue' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyMinSeaLevelPressureDate' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyMinSeaLevelPressureTime' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyTotalSeasonToDateHeatingDD' contient 13201 valeurs NaN.\n",
      "Colonne 'MonthlyTotalSeasonToDateCoolingDD' contient 13201 valeurs NaN.\n",
      "Colonne 'HOURLYSKYCONDITIONS' contient 5 valeurs NaN.\n",
      "Colonne 'HOURLYVISIBILITY' contient 5 valeurs NaN.\n",
      "Colonne 'HOURLYWETBULBTEMPF' contient 13 valeurs NaN.\n",
      "Colonne 'HOURLYWETBULBTEMPC' contient 13 valeurs NaN.\n",
      "Colonne 'HOURLYDewPointTempF' contient 4 valeurs NaN.\n",
      "Colonne 'HOURLYDewPointTempC' contient 4 valeurs NaN.\n",
      "Colonne 'HOURLYRelativeHumidity' contient 4 valeurs NaN.\n",
      "Colonne 'HOURLYStationPressure' contient 9 valeurs NaN.\n",
      "Colonne 'HOURLYSeaLevelPressure' contient 2 valeurs NaN.\n",
      "Colonne 'HOURLYPrecip' contient 5 valeurs NaN.\n",
      "Colonne 'DAILYAverageWindSpeed' contient 42 valeurs NaN.\n",
      "Colonne 'DAILYPeakWindSpeed' contient 115 valeurs NaN.\n"
     ]
    }
   ],
   "source": [
    "#We take only the data for the year 2017\n",
    "weather = dataframes['jfk_weather.csv']\n",
    "#print(weather.head())\n",
    "weather.drop(columns=['Unnamed: 0.1'], inplace=True)\n",
    "\n",
    "\n",
    "weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "weather_2017 = weather[weather['DATE'].dt.year == 2017]\n",
    "#print(weather_2017.head())\n",
    "#print(weather_2017.tail())\n",
    "#print(weather.info()) \n",
    "#90 columns\n",
    "\n",
    "\n",
    "\n",
    "#Columns with \"Monthly\", \"Hourly\" ou \"Daily\" contains only one value for the unit they represent.\n",
    "# for example for a monthly columns, only the last day of the corresponding month contains a value\n",
    "# We need to extend this\n",
    "\n",
    "# Extracting the columns \"Monthly\", \"Hourly\" ou \"Daily\"\n",
    "monthly_columns = [col for col in weather_2017.columns if 'Monthly' in col]\n",
    "hourly_columns = [col for col in weather_2017.columns if 'HOURLY' in col]\n",
    "daily_columns = [col for col in weather_2017.columns if 'DAILY' in col]\n",
    "\n",
    "# Conversion in datetime type\n",
    "weather_2017['YearMonth'] = weather_2017['DATE'].dt.to_period('M')  # Extraire l'année et le mois\n",
    "weather_2017['YearDayHour'] = weather_2017['DATE'].dt.to_period('H')  # Extraire l'année, jour et heure\n",
    "weather_2017['YearDay'] = weather_2017['DATE'].dt.to_period('D')  # Extraire l'année et jour\n",
    "\n",
    "# Filling the NaN values\n",
    "def fill_periodic_values(df, columns, period_key):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # Utiliser les groupes par période pour remplir les NaN avec ffill et bfill\n",
    "            df[col] = df.groupby(period_key)[col].transform(lambda group: group.ffill().bfill())\n",
    "\n",
    "fill_periodic_values(weather_2017, monthly_columns, 'YearMonth')\n",
    "fill_periodic_values(weather_2017, daily_columns, 'YearDay')\n",
    "fill_periodic_values(weather_2017, hourly_columns, 'YearDayHour')\n",
    "\n",
    "# Deleting the temporary columns\n",
    "weather_2017.drop(columns=['YearMonth', 'YearDayHour', 'YearDay'], inplace=True)\n",
    "\n",
    "\n",
    "# Veryfying other columns with NaN\n",
    "     \n",
    "# Some variables have value -9999 instead of NaN\n",
    "# We replace them by NaN to remove them later\n",
    "weather_2017 = weather_2017.replace(-9999, np.nan)\n",
    "\n",
    "check_nan_columns(weather_2017)\n",
    "\n",
    "# We remove the columns containg more than 1000 NaN values\n",
    "weather_2017 = weather_2017.dropna(axis=1, thresh=len(weather_2017) - 1000)\n",
    "check_nan_columns(weather_2017)\n",
    "#print(len(weather_2017)) #13201\n",
    "weather_2017 = weather_2017.dropna(axis=0)\n",
    "check_nan_columns(weather_2017) #nothing\n",
    "#print(len(weather_2017)) #13027\n",
    "\n",
    "\n",
    "#Deletion of useless columns (because of weather encoding standards (str whose meaning is not easily retrievable), complex units like angles, no variance, etc...)\n",
    "inutile = ['STATION','STATION_NAME','ELEVATION','LATITUDE','LONGITUDE', 'REPORTTPYE', 'HOURLYSKYCONDITIONS', 'HOURLYWindDirection', 'MonthlyDaysWithLT0Temp', 'DAILYSustainedWindDirection']\n",
    "weather_2017.drop(columns=inutile, inplace=True)\n",
    "\n",
    "#Hyptohesis: 'T' is used to indicate a quantity observed was too low to be measured, we assume it is equal to zero\n",
    "weather_2017 = weather_2017.replace('T', 0)\n",
    "\n",
    "#Some variables have values equal to a number followed by a character, we keep only the number by using regex\n",
    "for col in weather_2017.columns:\n",
    "    weather_2017[col] = weather_2017[col].replace(r'(\\d+(\\.\\d+)?)([^\\d\\s]+)$', r'\\1', regex=True)\n",
    "\n",
    "\n",
    "# We delete columns that represents the same thing but with different units (like celsius VS Farenheit)\n",
    "# we keep Fahrenheit because some variables do not have the celsius equivalent (American weather)\n",
    "Celsius = ['HOURLYDRYBULBTEMPC', 'HOURLYWETBULBTEMPC', 'HOURLYDewPointTempC']\n",
    "weather_2017.drop(columns=Celsius, inplace=True)\n",
    "\n",
    "#We set all variables to be float, expect time\n",
    "#delete the two first columns because it starts with Unnamed 0 and DATE to convert the rest to float\n",
    "weather_2017 = pd.concat([weather_2017[['DATE']], weather_2017.iloc[:, 2:].astype(float)], axis=1)\n",
    "\n",
    "#print(weather_2017.info())\n",
    "#print(weather_2017.head())\n",
    "\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/weather_2017.csv\", \"w\") as path:\n",
    "    weather_2017.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we merge the two datasets, and export specific dataframes for Data Exploration and Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of data time to numpy.datetime64 type to accelerate comparisons \n",
    "departure_times = JFK_2017_number['Full_Departure_Datetime'].values.astype('datetime64[m]')  # minutes\n",
    "weather_times = weather_2017['DATE'].values.astype('datetime64[m]')  # minutes\n",
    "\n",
    "#Hypothesis: the weather is the same for each 30-min interval of time\n",
    "tolerance = np.timedelta64(30, 'm')\n",
    "\n",
    "merged_rows = []\n",
    "\n",
    "#we want to find the minimum time difference between the plane departure and the measured weather\n",
    "for departure_time in departure_times:\n",
    "    time_differences = np.abs(departure_time - weather_times)\n",
    "    closest_index = np.argmin(time_differences)\n",
    "    \n",
    "    # If the time difference is beow the tolerance, we combine the information from JFK_numbers and weather_2017\n",
    "    if time_differences[closest_index] <= tolerance:\n",
    "        closest_weather_row = weather_2017.iloc[closest_index]\n",
    "        jfk_row = JFK_2017_number.iloc[np.where(departure_times == departure_time)[0][0]]  # find the corresponding row\n",
    "        merged_row = pd.concat([jfk_row, closest_weather_row], axis=0)\n",
    "        merged_rows.append(merged_row)\n",
    "\n",
    "# Combining all the rows that were accepted\n",
    "merged_df = pd.DataFrame(merged_rows, columns=np.concatenate([JFK_2017_number.columns, weather_2017.columns]))\n",
    "\n",
    "#print(merged_df['Full_Departure_Datetime'])\n",
    "#only about 10 rows were lost for a tolerance of 30min: acceptable \n",
    "\n",
    "# Uploading the data\n",
    "merged_df.rename(columns={'DATE': 'DATE_weather'}, inplace=True)\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/plane_weather.csv\", \"w\") as path:\n",
    "    merged_df.to_csv(path)\n",
    "\n",
    "check_nan_columns(merged_df)\n",
    "#print(merged_df.info())\n",
    "\n",
    "plane_weather_for_ML = merged_df.drop(columns=['Full_Departure_Datetime', 'DATE_weather'])\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/plane_weather_for_ML.csv\", \"w\") as path:\n",
    "    plane_weather_for_ML.to_csv(path)\n",
    "\n",
    "#print(plane_weather_for_ML.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Exploratory statistics**\n",
    "\n",
    "In this part, we analyze how the different variables behave and how they are linked. The goal is to pave the way to machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3: Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MODIFIER!!!!!!!!\n",
    "ICI JE PRENDS LE FICHIER DE MA LIBRAIRIE WORK ET PAS DE S3\n",
    "\n",
    "First, we load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Excel file format cannot be determined, you must specify an engine manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plane_weather \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../Pre-Processed_data/plane_weather.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plane_weather_for_ML \u001b[38;5;241m=\u001b[39m dataframes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplane_weather_for_ML.csv\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m JFK_2017_number \u001b[38;5;241m=\u001b[39m dataframes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJFK_2017_number.csv\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:1554\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[1;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m   1552\u001b[0m     )\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n\u001b[1;32m   1559\u001b[0m engine \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_option(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.excel.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.reader\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Excel file format cannot be determined, you must specify an engine manually."
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "plane_weather = pd.read_excel('../Pre-Processed_data/plane_weather.xlsx')\n",
    "plane_weather_for_ML = dataframes['plane_weather_for_ML.csv']\n",
    "JFK_2017_number = dataframes['JFK_2017_number.csv']\n",
    "weather_2017 = dataframes['weather_2017.csv']\n",
    "plane_weather.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "plane_weather_for_ML.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "JFK_2017_number.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "weather_2017.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4: Application**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
