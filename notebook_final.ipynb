{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting flight delays by weather data\n",
    "\n",
    "The goal of this notebook is to present a comprehensive analysis conducted as part of the \"Python for Data Scientists\" course at ENSAE. This project  includes data collection and preprocessing, visualization, scraping and modeling. By combining exploratory analysis and machine learning, the notebook aims to provide actionable insights into how weather affects flight schedules, focusing specifically on data from JFK Airport in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problematic\n",
    "Flight delays are a common inconvenience for both passengers and airlines, and while many are attributed to operational or logistical issues, weather often plays a crucial role. However, understanding and predicting these delays using weather data alone poses significant challenges.\n",
    "\n",
    "This project seeks to address the following key questions:\n",
    "- How do weather conditions correlate with flight delays?\n",
    "- Can weather data reliably predict delays?\n",
    "- What are the limitations of weather-based predictive models in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the project (and the notebook too)\n",
    "1. Data Collection and Cleaning\n",
    "2. Exploratory Data Analysis\n",
    "3. Modeling (ML)\n",
    "4. Interpretation and Insights\n",
    "5. Application that predicts flight delays with the previous models\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations and Prerequisites\n",
    "Before running the notebook, ensure that the required Python modules are installed to guarantee smooth execution of the code. Execute the following command in your terminal or notebook cell to install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Useful Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection to S3\n",
    "\n",
    "First, we download the files from leoacpr by typing the SSP Cloud username. If you do this step, please note that this can take a while because the files are heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "MY_BUCKET = \"leoacpr\"\n",
    "print(fs.ls(MY_BUCKET))\n",
    "source_folder = f\"{MY_BUCKET}/diffusion/Pre-processing\"\n",
    "files_in_source = fs.ls(source_folder)\n",
    "\n",
    "YOUR_BUCKET = str(input(\"Type your bucket: \\n\"))\n",
    "\n",
    "source_folder = f\"{MY_BUCKET}/diffusion/Pre-processing\"\n",
    "files_in_source = fs.ls(source_folder)\n",
    "\n",
    "# Copying the dataframe from leoacpr to your s3 database\n",
    "for file_path_in_s3 in files_in_source:\n",
    "    file_name = file_path_in_s3.split('/')[-1]  # Name of the file without the path\n",
    "\n",
    "    # If the file already exists in your database, then it won't download it\n",
    "    if fs.ls(f\"{MY_BUCKET}/diffusion/Pre-processing\") != fs.ls(f\"{YOUR_BUCKET}/diffusion/Pre-processing\"):\n",
    "        file_path_for_you = f\"{YOUR_BUCKET}/diffusion/Pre-processing/{file_name}\"\n",
    "        #import\n",
    "        with fs.open(file_path_in_s3, \"r\") as file_in:\n",
    "            df_imported = pd.read_csv(file_in)\n",
    "        #export\n",
    "        with fs.open(file_path_for_you, \"w\") as file_out:\n",
    "            df_imported.to_csv(file_out)\n",
    "        \n",
    "        print(f\"File {file_name} has been successfully copied to {file_path_for_you}\")\n",
    "\n",
    "#Create folders inside S3\n",
    "if not fs.exists(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data\"):\n",
    "    fs.touch(f\"{YOUR_BUCKET}/diffusion/.{Pre-processed_data}]\")\n",
    "\n",
    "#Downloading the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "for files in fs.ls(f\"{YOUR_BUCKET}/diffusion/Pre-processing\"):\n",
    "    with fs.open(files, \"r\") as file_in:\n",
    "            df_imported = pd.read_csv(file_in)\n",
    "            print(f\"Downloading {files}\")\n",
    "    # Dictionnary of dataframes with the name of the file as a key\n",
    "    dataframes[f\"{files.split('/')[-1]}\"] = df_imported\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will be useful after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_columns(df):\n",
    "    \"\"\"Checks and prints the columns containing NaN values\n",
    "\n",
    "    Args:\n",
    "        df (pd.dataframe): a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    nan_columns = df.columns[df.isna().any()].tolist()\n",
    "    for col in nan_columns:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        print(f\"Colonne '{col}' contient {nan_count} valeurs NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning (pre-processing)\n",
    "\n",
    "First part : Pre-processing the planes data\n",
    "\n",
    "Hypothesis: plane's delays are similarly sensitive to weather variations --> we can generalize the situation for an airport to others\n",
    "We decide to focus on JFK airport, whose identification number is 10135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Merging the monthly datasets to obtain a dataset for 2017\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m january_JFK \u001b[38;5;241m=\u001b[39m \u001b[43mdataframes\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_ONTIME_REPORTING_january.csv\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28;01mlambda\u001b[39;00m df: df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORIGIN_AIRPORT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10135\u001b[39m] \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(january_JFK)) \u001b[38;5;66;03m#191\u001b[39;00m\n\u001b[1;32m      4\u001b[0m february_JFK \u001b[38;5;241m=\u001b[39m dataframes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_ONTIME_REPORTING_february.csv\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28;01mlambda\u001b[39;00m df: df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORIGIN_AIRPORT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10135\u001b[39m] \n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "#Merging the monthly datasets to obtain a dataset for 2017\n",
    "january_JFK = dataframes['T_ONTIME_REPORTING_january.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(january_JFK)) #191\n",
    "february_JFK = dataframes['T_ONTIME_REPORTING_february.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(february_JFK)) #128\n",
    "march_JFK = dataframes['T_ONTIME_REPORTING_march.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(march_JFK)) #167\n",
    "april_JFK = dataframes['T_ONTIME_REPORTING_april.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(april_JFK)) #139\n",
    "may_JFK = dataframes['T_ONTIME_REPORTING_may.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(may_JFK)) #160\n",
    "june_JFK = dataframes['T_ONTIME_REPORTING_june.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(june_JFK)) #134\n",
    "july_JFK = dataframes['T_ONTIME_REPORTING_july.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(july_JFK)) #192\n",
    "august_JFK = dataframes['T_ONTIME_REPORTING_august.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(august_JFK)) #180\n",
    "september_JFK = dataframes['T_ONTIME_REPORTING_september.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(september_JFK)) #206\n",
    "october_JFK = dataframes['T_ONTIME_REPORTING_october.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(october_JFK)) #253\n",
    "november_JFK = dataframes['T_ONTIME_REPORTING_november.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(november_JFK)) #220\n",
    "december_JFK = dataframes['T_ONTIME_REPORTING_december.csv'][lambda df: df[\"ORIGIN_AIRPORT_ID\"] == 10135] \n",
    "print(len(december_JFK)) #168\n",
    "#Total size = 2138\n",
    "year = [january_JFK, february_JFK, march_JFK, april_JFK, may_JFK, june_JFK, july_JFK, august_JFK, september_JFK, october_JFK, november_JFK, december_JFK]\n",
    "JFK_2017 = pd.concat(year, ignore_index=True)\n",
    "JFK_2017.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "print(JFK_2017)\n",
    "#print(len(JFK_2017))\n",
    "\n",
    "\n",
    "#Setting the rights data types\n",
    "print(JFK_2017.info())\n",
    "JFK_2017['FL_DATE'] = pd.to_datetime(JFK_2017['FL_DATE'])\n",
    "\n",
    "\n",
    "#Hypothesis : we can replaces the missing values for delays by 0 because we suppose that a delay is more likely to be registered than no delay because it creates more frustration\n",
    "JFK_2017['WEATHER_DELAY'] = JFK_2017['WEATHER_DELAY'].fillna(0)\n",
    "JFK_2017['DEP_DELAY'] = JFK_2017['DEP_DELAY'].fillna(0)\n",
    "JFK_2017['CARRIER_DELAY'] = JFK_2017['CARRIER_DELAY'].fillna(0)\n",
    "JFK_2017['WEATHER_DELAY'] = JFK_2017['WEATHER_DELAY'].fillna(0)\n",
    "JFK_2017['ARR_DELAY'] = JFK_2017['ARR_DELAY'].fillna(0)\n",
    "\n",
    "#Removing NaN per rows\n",
    "check_nan_columns(JFK_2017)\n",
    "#'DEP_TIME' has 36  NaN.\n",
    "#'ARR_TIME' has 39 valeurs NaN.\n",
    "JFK_2017 = JFK_2017.dropna(axis=0)\n",
    "check_nan_columns(JFK_2017) #nothing --> no more NaN\n",
    "print(len(JFK_2017)) #1919\n",
    "\n",
    "\n",
    "#Combining departure time information to obtain a column with year, month, day, hour, minute for departure\n",
    "#'DEP_TIME' contains str with numbers indicating the hours and minutes through the format \"hhmm\"\n",
    "JFK_2017['DEP_TIME'] = JFK_2017['DEP_TIME'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "JFK_2017['DEP_TIME'] = JFK_2017['DEP_TIME'].astype(str).str.zfill(4)  # Ensure it's 4 digits\n",
    "JFK_2017['Hours'] = JFK_2017['DEP_TIME'].str[:2].astype(int)  # Get hours as integer\n",
    "JFK_2017['Minutes'] = JFK_2017['DEP_TIME'].str[2:].astype(int)  # Get minutes as integer\n",
    "JFK_2017['departure_time'] = pd.to_timedelta(JFK_2017['Hours'], unit='h') + pd.to_timedelta(JFK_2017['Minutes'], unit='m')\n",
    "JFK_2017['Full_Departure_Datetime'] = JFK_2017['FL_DATE'] + JFK_2017['departure_time']\n",
    "JFK_2017.drop(['Hours', 'Minutes', 'departure_time'], axis=1, inplace=True)\n",
    "print(JFK_2017[['FL_DATE', 'DEP_TIME', 'Full_Departure_Datetime']].head())\n",
    "\n",
    "#Isolating the data for machine learning \n",
    "JFK_2017_no_number = JFK_2017[['Full_Departure_Datetime', 'FL_DATE','OP_UNIQUE_CARRIER','OP_CARRIER_AIRLINE_ID','OP_CARRIER','TAIL_NUM','OP_CARRIER_FL_NUM','ORIGIN_AIRPORT_ID','ORIGIN_AIRPORT_SEQ_ID','ORIGIN_CITY_MARKET_ID','DEST_AIRPORT_ID','DEST_CITY_MARKET_ID','DEST', 'DEP_TIME','ARR_TIME']]\n",
    "JFK_2017_number = JFK_2017[['DEP_DELAY','ARR_DELAY','CANCELLED','CARRIER_DELAY','WEATHER_DELAY','Full_Departure_Datetime']]\n",
    "JFK_2017_number['CANCELLED'] = JFK_2017_number['CANCELLED'].astype(int)\n",
    "print(JFK_2017_number.info())\n",
    "\n",
    "#Exporting the dataset for JFK planes\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/JFK_2017.csv\", \"w\") as path:\n",
    "    JFK_2017.to_csv(path)\n",
    "\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/JFK_2017_no_number.csv\", \"w\") as path:\n",
    "    JFK_2017_no_number.to_csv(path)\n",
    "\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/JFK_2017_number.csv\", \"w\") as path:\n",
    "    JFK_2017_number.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing the weather data, we do the same thing but with the weather data from JFK airport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#We take only the data for the year 2017\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m weather \u001b[38;5;241m=\u001b[39m \u001b[43mdataframes\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjfk_weather.csv\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m weather\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m weather[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(weather[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "#We take only the data for the year 2017\n",
    "weather = dataframes['jfk_weather.csv']\n",
    "weather.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "\n",
    "weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "weather_2017 = weather[weather['DATE'].dt.year == 2017]\n",
    "#print(weather_2017.head())\n",
    "#print(weather_2017.tail())\n",
    "print(weather.info()) \n",
    "#90 columns\n",
    "\n",
    "\n",
    "\n",
    "#Columns with \"Monthly\", \"Hourly\" ou \"Daily\" contains only one value for the unit they represent.\n",
    "# for example for a monthly columns, only the last day of the corresponding month contains a value\n",
    "# We need to extend this\n",
    "\n",
    "# Extracting the columns \"Monthly\", \"Hourly\" ou \"Daily\"\n",
    "monthly_columns = [col for col in weather_2017.columns if 'Monthly' in col]\n",
    "hourly_columns = [col for col in weather_2017.columns if 'HOURLY' in col]\n",
    "daily_columns = [col for col in weather_2017.columns if 'DAILY' in col]\n",
    "\n",
    "# Conversion in datetime type\n",
    "weather_2017['YearMonth'] = weather_2017['DATE'].dt.to_period('M')  # Extraire l'année et le mois\n",
    "weather_2017['YearDayHour'] = weather_2017['DATE'].dt.to_period('H')  # Extraire l'année, jour et heure\n",
    "weather_2017['YearDay'] = weather_2017['DATE'].dt.to_period('D')  # Extraire l'année et jour\n",
    "\n",
    "# Filling the NaN values\n",
    "def fill_periodic_values(df, columns, period_key):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # Utiliser les groupes par période pour remplir les NaN avec ffill et bfill\n",
    "            df[col] = df.groupby(period_key)[col].transform(lambda group: group.ffill().bfill())\n",
    "\n",
    "fill_periodic_values(weather_2017, monthly_columns, 'YearMonth')\n",
    "fill_periodic_values(weather_2017, daily_columns, 'YearDay')\n",
    "fill_periodic_values(weather_2017, hourly_columns, 'YearDayHour')\n",
    "\n",
    "# Deleting the temporary columns\n",
    "weather_2017.drop(columns=['YearMonth', 'YearDayHour', 'YearDay'], inplace=True)\n",
    "\n",
    "\n",
    "# Veryfying other columns with NaN\n",
    "     \n",
    "# Some variables have value -9999 instead of NaN\n",
    "# We replace them by NaN to remove them later\n",
    "weather_2017 = weather_2017.replace(-9999, np.nan)\n",
    "\n",
    "check_nan_columns(weather_2017)\n",
    "\n",
    "# We remove the columns containg more than 1000 NaN values\n",
    "weather_2017 = weather_2017.dropna(axis=1, thresh=len(weather_2017) - 1000)\n",
    "check_nan_columns(weather_2017)\n",
    "#print(len(weather_2017)) #13201\n",
    "weather_2017 = weather_2017.dropna(axis=0)\n",
    "check_nan_columns(weather_2017) #nothing\n",
    "#print(len(weather_2017)) #13027\n",
    "\n",
    "\n",
    "#Deletion of useless columns (because of weather encoding standards (str whose meaning is not easily retrievable), complex units like angles, no variance, etc...)\n",
    "inutile = ['STATION','STATION_NAME','ELEVATION','LATITUDE','LONGITUDE', 'REPORTTPYE', 'HOURLYSKYCONDITIONS', 'HOURLYWindDirection', 'MonthlyDaysWithLT0Temp', 'DAILYSustainedWindDirection']\n",
    "weather_2017.drop(columns=inutile, inplace=True)\n",
    "\n",
    "#Hyptohesis: 'T' is used to indicate a quantity observed was too low to be measured, we assume it is equal to zero\n",
    "weather_2017 = weather_2017.replace('T', 0)\n",
    "\n",
    "#Some variables have values equal to a number followed by a character, we keep only the number by using regex\n",
    "for col in weather_2017.columns:\n",
    "    weather_2017[col] = weather_2017[col].replace(r'(\\d+(\\.\\d+)?)([^\\d\\s]+)$', r'\\1', regex=True)\n",
    "\n",
    "\n",
    "# We delete columns that represents the same thing but with different units (like celsius VS Farenheit)\n",
    "# we keep Fahrenheit because some variables do not have the celsius equivalent (American weather)\n",
    "Celsius = ['HOURLYDRYBULBTEMPC', 'HOURLYWETBULBTEMPC', 'HOURLYDewPointTempC']\n",
    "weather_2017.drop(columns=Celsius, inplace=True)\n",
    "\n",
    "#We set all variables to be float, expect time\n",
    "weather_2017 = pd.concat([weather_2017[['DATE']], weather_2017.iloc[:, 1:].astype(float)], axis=1)\n",
    "\n",
    "print(weather_2017.info())\n",
    "print(weather_2017.head())\n",
    "\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/weather_2017.csv\", \"w\") as path:\n",
    "    weather_2017.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we merge the two datasets, and export a specific dataframe for the Data Exploration, and one for the Machine Learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of data time to numpy.datetime64 type to accelerate comparisons \n",
    "departure_times = JFK_2017_number['Full_Departure_Datetime'].values.astype('datetime64[m]')  # minutes\n",
    "weather_times = weather_2017['DATE'].values.astype('datetime64[m]')  # minutes\n",
    "\n",
    "#Hypothesis: the weather is the same for each 30-min interval of time\n",
    "tolerance = np.timedelta64(30, 'm')\n",
    "\n",
    "merged_rows = []\n",
    "\n",
    "#we want to find the minimum time difference between the plane departure and the measured weather\n",
    "for departure_time in departure_times:\n",
    "    time_differences = np.abs(departure_time - weather_times)\n",
    "    closest_index = np.argmin(time_differences)\n",
    "    \n",
    "    # If the time difference is beow the tolerance, we combine the information from JFK_numbers and weather_2017\n",
    "    if time_differences[closest_index] <= tolerance:\n",
    "        closest_weather_row = weather_2017.iloc[closest_index]\n",
    "        jfk_row = JFK_2017_number.iloc[np.where(departure_times == departure_time)[0][0]]  # find the corresponding row\n",
    "        merged_row = pd.concat([jfk_row, closest_weather_row], axis=0)\n",
    "        merged_rows.append(merged_row)\n",
    "\n",
    "# Combining all the rows that were accepted\n",
    "merged_df = pd.DataFrame(merged_rows, columns=np.concatenate([JFK_2017_number.columns, weather_2017.columns]))\n",
    "\n",
    "print(merged_df['Full_Departure_Datetime'])\n",
    "#only about 10 rows were lost for a tolerance of 31min: acceptable \n",
    "\n",
    "# Uploading the data\n",
    "merged_df.rename(columns={'DATE': 'DATE_weather'}, inplace=True)\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/plane_weather.csv\", \"w\") as path:\n",
    "    merged_df.to_csv(path)\n",
    "\n",
    "check_nan_columns(merged_df)\n",
    "print(merged_df.info())\n",
    "\n",
    "plane_weather_for_ML = merged_df.drop(columns=['Full_Departure_Datetime', 'DATE_weather'])\n",
    "with fs.open(f\"{YOUR_BUCKET}/diffusion/Pre-processed_data/plane_weather_for_ML.csv\", \"w\") as path:\n",
    "    plane_weather_for_ML.to_csv(path)\n",
    "\n",
    "print(plane_weather_for_ML.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MODIFIER!!!!!!!!\n",
    "ICI JE PRENDS LE FICHIER DE MA LIBRAIRIE WORK ET PAS DE S3\n",
    "\n",
    "First, we load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Excel file format cannot be determined, you must specify an engine manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plane_weather \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../Pre-Processed_data/plane_weather.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plane_weather_for_ML \u001b[38;5;241m=\u001b[39m dataframes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplane_weather_for_ML.csv\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m JFK_2017_number \u001b[38;5;241m=\u001b[39m dataframes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJFK_2017_number.csv\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/io/excel/_base.py:1554\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[1;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m   1552\u001b[0m     )\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n\u001b[1;32m   1559\u001b[0m engine \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_option(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.excel.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.reader\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Excel file format cannot be determined, you must specify an engine manually."
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "plane_weather = pd.read_excel('../Pre-Processed_data/plane_weather.xlsx')\n",
    "plane_weather_for_ML = dataframes['plane_weather_for_ML.csv']\n",
    "JFK_2017_number = dataframes['JFK_2017_number.csv']\n",
    "weather_2017 = dataframes['weather_2017.csv']\n",
    "plane_weather.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "plane_weather_for_ML.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "JFK_2017_number.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "weather_2017.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
